## Topic detecion

We ran a Latent Dirichlet Allocation topic detection algorithm in order to get some sense of what clusters are about. LDA assumes that each document is a mixture of a small number of latent topics, and that each word in the document is generated from one of the topics. It is an unsupervised algorithm that discovers topics in the text. However, LDA produces a list of most significant words that describes a topic, such as topic-0: US, immigration, guns, violence, protests, police, arrest wall. Instead, We were interested in producing a single label for each topic: instead of "topic-0" we wish to generate "US immigration". To this end, we use "NETL-Automatic-Topic-Labelling" from https://github.com/sb1992/NETL-Automatic-Topic-Labelling- , an automatic topic labelling model that uses the output from LDA and generates 19 candidate labels from pretrained word2vec and doc2vec. Afterwards, we have the choice of using unsupervised or supervised selection of top-k best candidate labels. For sake of simplicity we have chosen to return best 3 candidate labels for each topic. 

We ran LDA and NETL over the description of the videos and the titles. We have found that using a video's description as an input to the model does not produce meaningful topics, because videos' description is often filled with hyperlinks or repeatable content. Thus we use video's title as a main source for extracting topics. LDA performed extremely well producing meaningful topics. However, NETL model produces very poor results. The main reason is the outdates pertained word2vec and doc2vec models. We did not succeed to find new pertained models. 

In order to get an idea of cluster topic, we manually inspect LDA's produced terms and get sense of their most visible topics for each cluster.
