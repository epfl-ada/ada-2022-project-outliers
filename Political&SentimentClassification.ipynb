{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "XDLhQIf2k7dn",
        "PZ59a3_xBQCe",
        "RGa_leBwjPW0"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        " ***Political & Sentiment Classification***\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        " *Introduction*\n",
        " \n",
        "> The goal of this notebook is to:\n",
        "1.   Train a model for sentiment analysis based of the title and text description of Youtube videos.\n",
        "2.   Train a model for political classification based of title and text description of Youtube videos\n",
        "\n",
        "*Summary*\n",
        "\n",
        "> Since we are dealing with a NLP problem, we decided to use the pretrained model BERT, more exactly [PolitBERT ](https://huggingface.co/maurice/PolitBERT) (BERT flavour specialized on political speeches, interviews and press briefings of English-speaking politicians), since we only consider videos from News&Politics category.\n",
        "\n",
        "> We then fine-tuned the model for our specific task, using various datasets to capture the specificity of what we were looking for.\n",
        "\n",
        "\n",
        "\n",
        "> ***Sentiment Analysis Model***\n",
        "\n",
        "> For the Sentiment Analysis Model, we used the \n",
        "[Twitter and Reddit Sentimental analysis Dataset](https://www.kaggle.com/datasets/cosmos98/twitter-and-reddit-sentimental-analysis-dataset), in order to capture how people are talking online.\n",
        "\n",
        "\n",
        "\n",
        "> ***Political Classification***\n",
        "\n",
        "> For the Political Classification , we used the \n",
        "[Democrat Vs. Republican Tweets](https://www.kaggle.com/datasets/kapastor/democratvsrepublicantweets), in order to capture how political ideologies and affiliations are expressed online.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "sETjyximeUAS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "XDLhQIf2k7dn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import re\n",
        "import copy\n",
        "from tqdm.notebook import tqdm\n",
        "import gc\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from transformers import (\n",
        "    AutoTokenizer, \n",
        "    AutoModel,\n",
        "    get_linear_schedule_with_warmup\n",
        ")\n",
        "\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    f1_score\n",
        ")\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from transformers import BertTokenizer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_t0sRv_KlNon",
        "outputId": "80c09499-af0a-4b9d-c666-1c8950ab4e45"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 5.5 MB 7.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 7.6 MB 54.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 182 kB 54.1 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Process the text"
      ],
      "metadata": {
        "id": "PZ59a3_xBQCe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pre-process the text.\n",
        "# Remove punctuation marks and trailing spaces from text.\n",
        "def clean_and_parse_text(text):\n",
        "    if type(text) is not str:\n",
        "      text = ''\n",
        "\n",
        "    text = text.split()\n",
        "    text = [x.strip().lower() for x in text]\n",
        "    text = [x.replace('\\n', ' ').replace('\\t', ' ') for x in text]\n",
        "    text = ' '.join(text)\n",
        "    text = re.sub('([.,!?()])', r' \\1 ', text)\n",
        "    return text\n",
        "\n",
        "# Get the text from the dataframe and process it.\n",
        "def get_texts(df):\n",
        "    texts = df.apply(lambda x: clean_and_parse_text(x))\n",
        "    texts = texts.values.tolist()\n",
        "   \n",
        "    return texts"
      ],
      "metadata": {
        "id": "IaQOyDd9BPbM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the Data used.\n",
        "class TransformerDataset(Dataset):\n",
        "  def __init__(self, df, labels=None, set_type=None):\n",
        "    super(TransformerDataset, self).__init__()\n",
        "\n",
        "    self.texts = get_texts(df)\n",
        "    \n",
        "    self.set_type = set_type\n",
        "    if self.set_type != 'test':\n",
        "      self.labels = labels\n",
        "    \n",
        "    self.tokenizer = config.TOKENIZER\n",
        "    self.max_length = config.MAX_LENGTH\n",
        "\n",
        "  def __len__(self):\n",
        "      return len(self.texts)\n",
        "    \n",
        "  def __getitem__(self, index):\n",
        "    tokenized = self.tokenizer.encode_plus(\n",
        "        self.texts[index], \n",
        "        max_length=self.max_length,\n",
        "        pad_to_max_length=True,\n",
        "        truncation=True,\n",
        "        return_attention_mask=True,\n",
        "        return_token_type_ids=False,\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "    input_ids = tokenized['input_ids'].squeeze()\n",
        "    attention_mask = tokenized['attention_mask'].squeeze()\n",
        "\n",
        "    # For training, we also need the labels.\n",
        "    if self.set_type != 'test':\n",
        "      return {\n",
        "          'input_ids': input_ids.long(),\n",
        "          'attention_mask': attention_mask.long(),\n",
        "          'labels': torch.Tensor(self.labels[index]),\n",
        "      }\n",
        "\n",
        "    return {\n",
        "        'input_ids': input_ids.long(),\n",
        "        'attention_mask': attention_mask.long(),\n",
        "    }"
      ],
      "metadata": {
        "id": "Md-T8atqQyaB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Configurations"
      ],
      "metadata": {
        "id": "fealktVZQIfU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Used parameters to train the models.\n",
        "class Config:\n",
        "  def __init__(self):\n",
        "    super(Config, self).__init__()\n",
        "\n",
        "    self.SEED = 42\n",
        "    self.MODEL_PATH = 'maurice/PolitBERT'\n",
        "    self.NUMBER_POLITICAL_PARTIES = 2\n",
        "    self.NUMBER_EMOTIONS = 3\n",
        "\n",
        "    # data\n",
        "    self.TOKENIZER = AutoTokenizer.from_pretrained(self.MODEL_PATH)\n",
        "    self.MAX_LENGTH = 320\n",
        "    self.BATCH_SIZE = 16\n",
        "    self.VALIDATION_SPLIT = 0.25\n",
        "\n",
        "     # model\n",
        "    self.DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    self.FULL_FINETUNING = True\n",
        "    self.LR = 3e-5\n",
        "    self.OPTIMIZER = 'AdamW'\n",
        "    self.CRITERION = 'BCEWithLogitsLoss'\n",
        "    self.N_VALIDATE_DUR_TRAIN = 3\n",
        "    self.N_WARMUP = 0\n",
        "    self.SAVE_BEST_ONLY = True\n",
        "    self.EPOCHS = 50\n",
        "\n",
        "config = Config()"
      ],
      "metadata": {
        "id": "H8qJkJ2cQNJ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define the generic model"
      ],
      "metadata": {
        "id": "agDzBa8ObT_3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the layers of the model. \n",
        "class TransformerModel(nn.Module):\n",
        "  def __init__(self, transformer_model: AutoModel, classes: int):\n",
        "    super(TransformerModel, self).__init__()\n",
        "    self.transformer = transformer_model\n",
        "    self.output = nn.Linear(768, classes)\n",
        "\n",
        "  def forward(self, input_ids, attention_mask=None, token_type_ids=None):\n",
        "    _, o2 = self.transformer(\n",
        "      input_ids=input_ids,\n",
        "      attention_mask=attention_mask,\n",
        "      token_type_ids=token_type_ids,\n",
        "      return_dict=False\n",
        "    )\n",
        "    x = self.output(o2)\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "4U9lT5z2cdRg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generic functions for training the model"
      ],
      "metadata": {
        "id": "mOfnwMRMh1Z8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = config.DEVICE\n",
        "device"
      ],
      "metadata": {
        "id": "Qp7Ij9fnhkGW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the current model\n",
        "def val(model, val_dataloader, criterion):\n",
        "    \n",
        "    val_loss = 0\n",
        "    true, pred = [], []\n",
        "    \n",
        "    # Set model.eval() every time during evaluation\n",
        "    model.eval()\n",
        "    \n",
        "    for step, batch in enumerate(val_dataloader):\n",
        "        # Unpack the batch contents and push them to the device (cuda or cpu).\n",
        "        b_input_ids = batch['input_ids'].to(device)\n",
        "        b_attention_mask = batch['attention_mask'].to(device)\n",
        "        b_labels = batch['labels'].to(device)\n",
        "\n",
        "        # Using torch.no_grad() during validation/inference is faster \n",
        "        # since it does not update gradients.\n",
        "        with torch.no_grad():\n",
        "            # Forward pass\n",
        "            logits = model(input_ids=b_input_ids, attention_mask=b_attention_mask)\n",
        "            \n",
        "            # Calculate loss\n",
        "            loss = criterion(logits, b_labels)\n",
        "            val_loss += loss.item()\n",
        "             \n",
        "            # Since we're using BCEWithLogitsLoss, to get the predictions \n",
        "            # sigmoid has to be applied on the logits first\n",
        "            logits = torch.sigmoid(logits)\n",
        "            \n",
        "            logits = np.round(logits.cpu().numpy())\n",
        "            \n",
        "            labels = b_labels.cpu().numpy()\n",
        "            \n",
        "            # The tensors are detached from the gpu and put back on \n",
        "            # the cpu, and then converted to numpy in order to \n",
        "            # use sklearn's metrics.\n",
        "            pred.extend(logits)\n",
        "            true.extend(labels)\n",
        "\n",
        "    avg_val_loss = val_loss / len(val_dataloader)\n",
        "    print('Eval Val loss:', avg_val_loss)\n",
        "    print('Eval Val accuracy:', accuracy_score(true, pred))\n",
        "    \n",
        "    \n",
        "    val_micro_f1_score = f1_score(true, pred, average='micro')\n",
        "    print('Eval Val micro f1 score:', val_micro_f1_score)\n",
        "    return val_micro_f1_score\n",
        "\n",
        "def train(model, train_dataloader, val_dataloader, criterion, optimizer, scheduler, epoch):\n",
        "    \n",
        "    # Validate config.N_VALIDATE_DUR_TRAIN times during the training loop\n",
        "    nv = config.N_VALIDATE_DUR_TRAIN\n",
        "    temp = len(train_dataloader) // nv\n",
        "    \n",
        "    if temp > 100:\n",
        "      temp = temp - (temp % 100)\n",
        "    validate_at_steps = [temp * x for x in range(1, nv + 1)]\n",
        "    \n",
        "    train_loss = 0\n",
        "    for step, batch in enumerate(tqdm(train_dataloader, \n",
        "                                      desc='Epoch ' + str(epoch))):\n",
        "        # Set model.eval() every time during training\n",
        "        model.train()\n",
        "        \n",
        "        # Unpack the batch contents and push them to the device (cuda or cpu).\n",
        "        b_input_ids = batch['input_ids'].to(device)\n",
        "        b_attention_mask = batch['attention_mask'].to(device)\n",
        "        b_labels = batch['labels'].to(device)\n",
        "\n",
        "        # Clear accumulated gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        logits = model(input_ids=b_input_ids, attention_mask=b_attention_mask)\n",
        "        \n",
        "        # Calculate loss\n",
        "        loss = criterion(logits, b_labels)\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "\n",
        "        # Update weights\n",
        "        optimizer.step()\n",
        "        \n",
        "        # Update scheduler\n",
        "        scheduler.step()\n",
        "\n",
        "        if step in validate_at_steps:\n",
        "            print(f'-- Step: {step}')\n",
        "            _ = val(model, val_dataloader, criterion)\n",
        "    \n",
        "    avg_train_loss = train_loss / len(train_dataloader)\n",
        "    print('Training loss:', avg_train_loss)\n"
      ],
      "metadata": {
        "id": "_58dBizZhe1J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run(model, train_dataloader, val_dataloader, model_name):\n",
        "    # Setting a seed ensures reproducible results.\n",
        "    # Seed may affect the performance too.\n",
        "    torch.manual_seed(config.SEED)\n",
        "\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "    \n",
        "    # Define the parameters to be optmized \n",
        "    # and add regularization\n",
        "    if config.FULL_FINETUNING:\n",
        "        param_optimizer = list(model.named_parameters())\n",
        "        no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
        "        optimizer_parameters = [\n",
        "            {\n",
        "                \"params\": [\n",
        "                    p for n, p in param_optimizer if not any(nd in n for nd in no_decay)\n",
        "                ],\n",
        "                \"weight_decay\": 0.001,\n",
        "            },\n",
        "            {\n",
        "                \"params\": [\n",
        "                    p for n, p in param_optimizer if any(nd in n for nd in no_decay)\n",
        "                ],\n",
        "                \"weight_decay\": 0.0,\n",
        "            },\n",
        "        ]\n",
        "        optimizer = optim.AdamW(optimizer_parameters, lr=config.LR)\n",
        "    \n",
        "    num_training_steps = len(train_dataloader) * config.EPOCHS\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer,\n",
        "        num_warmup_steps=0,\n",
        "        num_training_steps=num_training_steps\n",
        "    )\n",
        "    \n",
        "    max_val_micro_f1_score = float('-inf')\n",
        "    for epoch in range(config.EPOCHS):\n",
        "        train(model, train_dataloader, val_dataloader, criterion, optimizer, scheduler, epoch)\n",
        "        val_micro_f1_score = val(model, val_dataloader, criterion)\n",
        "        print(\"Epoch \" + str(epoch) + \"/\" + str(config.EPOCHS) + \": F1 Score \" + str(val_micro_f1_score))\n",
        "        if config.SAVE_BEST_ONLY:\n",
        "            if val_micro_f1_score > max_val_micro_f1_score:\n",
        "                best_model = copy.deepcopy(model)\n",
        "                best_val_micro_f1_score = val_micro_f1_score\n",
        "\n",
        "                torch.save(best_model.state_dict(), model_name + '.pt')\n",
        "\n",
        "                print(f'--- Best Model. Val loss: {max_val_micro_f1_score} -> {val_micro_f1_score}')\n",
        "                max_val_micro_f1_score = val_micro_f1_score\n",
        "\n",
        "    return best_model, best_val_micro_f1_score\n"
      ],
      "metadata": {
        "id": "C58U9iPLiCfi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sentiment Analysis"
      ],
      "metadata": {
        "id": "MqiXRG40gos1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get the training data"
      ],
      "metadata": {
        "id": "RGa_leBwjPW0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"Reddit_Data.csv\")\n",
        "X_sent = df['clean_comment']\n",
        "y_sent = pd.DataFrame(df['category'])"
      ],
      "metadata": {
        "id": "A4mJ6CGDjRps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pre-process the training data"
      ],
      "metadata": {
        "id": "iIomnikdjaEw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a column for each sentiment. \n",
        "# Each snetiment will be mapped for an individual node in the final layer.\n",
        "y_sent['-1'] = int(y_sent['category'] == -1)\n",
        "y_sent['0'] = int(y_sent['category'] == 0)\n",
        "y_sent['1'] = int(y_sent['category'] == 1)\n",
        "y_sent = y_sent.drop('category', axis = 1)"
      ],
      "metadata": {
        "id": "YkE5tbpAjOjz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Split dataset into train and test"
      ],
      "metadata": {
        "id": "uYvyQLYkjq8b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_sent, X_test_sent, y_train_sent, y_test_sent = train_test_split(X_sent, y_sent, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "FWj12uJAjtjI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train the model"
      ],
      "metadata": {
        "id": "UPjhEtrMj0k7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transformer_weights = AutoModel.from_pretrained(\n",
        "  config.MODEL_PATH\n",
        ")\n",
        "sentiment_analysis = TransformerModel(transformer_weights, config.NUMBER_EMOTIONS)"
      ],
      "metadata": {
        "id": "RStO__IkgrJh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the model on GPU\n",
        "sentiment_analysis.to(device);"
      ],
      "metadata": {
        "id": "UKF80i2YjtqX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "22dfb1d8f7ef4466a77f54f8f38d2cc9",
            "3d64e1e0fe3141caae1e80a3cb643d3b",
            "7cb63aa595f64034b14fb4927422ff6e",
            "b486e8c7b0674d69a168300605e3a658",
            "409d78f3b63b42168a1d50b2bfa4640e",
            "6833a7c23c27471ebaa011f082642170",
            "e5f59d6a10d24e4d8a446fce64a03ef7",
            "6996d56957b346198f2869ae93a5f791",
            "1ad72a678b8d47a68fd1cdb44df5c40d",
            "63f683c3fc7a494b975b00292079219f",
            "66e3c27614a847a593ff585bf147922f",
            "a0c8c818e6ca48cc89faf9abd1c38d2a",
            "b58046bb88f34bef9f54b663dce57b44",
            "8db16d17c513499fabe24ded8971f37e",
            "d5ed63ba6c4c4487b56647d9171cd712",
            "f4e35d60729f4a2b8f09e1483d3f19e1",
            "95873e81b4d147f1b084d5bf4db10ae0",
            "78f8e0f4ebe349b7b764f503b8c38db7",
            "552d2e20df134215af2575d4d59e80cd",
            "8ce470b368a04cefb501a52dc5359499",
            "aea32e95b3d54d4b8be380c8f130e4fa",
            "4b2db8848a354e8394240f15cf56b8f0",
            "72099dc3a5734858bd059f0706bedda8",
            "a5233240c32e43f09b088c9afe74ca15",
            "b2c906b2cf804434a29a7a1945edbdb0",
            "478116386c664f28867541d08231e2f1",
            "e2a2a71d2fd44ae9a6f7d5f70495af67",
            "e7ee0d01fa374dafac133dcf3f68ce84",
            "32807cf6247a45bf9dd51218f8ddcf5a",
            "17cb42d23e8b4c8d87dbb113e2cf229f",
            "7044d5fd67c847c5937bfb1db77c5711",
            "b134e8555cc6407db97d6ca9172e07ab",
            "b1271f7714824e60817652a9ce12479e",
            "78b72b59325d46d1bbe4b4538b1413ce",
            "c072d3d8ffb442af843d63368ecc3749",
            "1c6cb3d78fe84cd1a5f2f2858f4aa166",
            "f9d15ae5ac94431883ea916ae7df2327",
            "ffce979329cc4ff3ab9e9eb6a6b93360",
            "42949ee1e02e44e29c66411b7f9c4eb1",
            "d21c7e0fe5404b058d46bb38d13d6aea",
            "f82f8efdce2a4d1d9cc14f24d6b691a1",
            "86e194e0059744e6890f7b4ce0fa8adc",
            "db7572a551c24346adddbfa83527fe4c",
            "7b1be5314b254f20948b5017a282b4aa",
            "2ffd7394bf5d417584f59a4da689841c",
            "43067b34bfac43c0ad4f1bf6ff6029a7",
            "4642d36697a34a818289ff2b3c372be0",
            "b61f926e1afe4946a2c1b680fb2ccfb5",
            "81d991c42dbf4e6f977124ef237ddc8c",
            "26e83afb3fa14e45ae375c2a1cd83636",
            "0efb11f2a71f4ef2a7e13555950df2c2",
            "f3070c30a880498e93a00744392c7de8",
            "d1bc2db3d20f4d8ea04896b361914711",
            "d471c356831f47f888c580653c83bc7c",
            "17ecbdfefc324b74a0049a26d5c0e077",
            "1d9cabfbf05845edb0fa449332735433",
            "3533a9349b264399bbe75c415ead24df",
            "54aa366bf89d4ed5abaf355e7e3861d7",
            "6c00194a67eb4f6a91a639fef41b10f5",
            "d769aa61f8cb404ebbe2be0647dc3926",
            "67aaa63274554f62bcf861d2ef8a0857",
            "c5f4ccf4e4204b54a822db783ef07480",
            "e8d6d2e9edc347be8a2444ffa70cc023",
            "049162e0b05c4314afa0a92321b674e1",
            "c902a5a6aba449149524acafda5e79b1",
            "7a84627e44e340949e5f040e82101ddb",
            "72671dd83c1c4e34945ee6fed2d8ea99",
            "7accb31ea2624626ab632f0f88eeb437",
            "3de7577819f1437e9ee31e3f3ce21cca",
            "31cbf0cfe68a4e46ae02208125031343",
            "b8d099716406423bb8e4dd4595ffb82a",
            "760e1a98470740779c06c904059a02fa",
            "0771e87b8f604f9b98c1dd5324f8fdd9",
            "f1182ac3dafa4931b132aadc68023fa4",
            "d52f3fc5c9ae4733accb96272664fa05",
            "2086a300a0d64a1f883d390e236d6a61",
            "b57c879bcfb94822bf5decda7c9282c3",
            "1a29acb0b770492094f985091b1e0b5e",
            "67c5bd845c844d168abf1fb48502ddb5",
            "53dcb0c1c74442efa32840a2a5052e44"
          ]
        },
        "outputId": "8e0edbe3-b565-43f3-a7c9-371675f78681",
        "id": "-j6AIm-fmV8H"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  \n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "22dfb1d8f7ef4466a77f54f8f38d2cc9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Epoch 0:   0%|          | 0/1863 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-- Step: 600\n",
            "Eval Val loss: 0.25957285150757675\n",
            "Eval Val accuracy: 0.8276510067114093\n",
            "Eval Val micro f1 score: 0.8487958401751506\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-- Step: 1200\n",
            "Eval Val loss: 0.18081660813536063\n",
            "Eval Val accuracy: 0.8981208053691275\n",
            "Eval Val micro f1 score: 0.9037656903765691\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-- Step: 1800\n",
            "Eval Val loss: 0.14386240484057067\n",
            "Eval Val accuracy: 0.9178523489932886\n",
            "Eval Val micro f1 score: 0.9239773950484391\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training loss: 0.25981179659389325\n",
            "Eval Val loss: 0.14748089009887927\n",
            "Eval Val accuracy: 0.9118120805369128\n",
            "Eval Val micro f1 score: 0.9203825946382863\n",
            "Epoch 0/50: F1 Score 0.9203825946382863\n",
            "--- Best Model. Val loss: -inf -> 0.9203825946382863\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a0c8c818e6ca48cc89faf9abd1c38d2a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Epoch 1:   0%|          | 0/1863 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-- Step: 600\n",
            "Eval Val loss: 0.13027490015829166\n",
            "Eval Val accuracy: 0.9284563758389262\n",
            "Eval Val micro f1 score: 0.933144628656153\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-- Step: 1200\n",
            "Eval Val loss: 0.1305654987666852\n",
            "Eval Val accuracy: 0.9256375838926174\n",
            "Eval Val micro f1 score: 0.9312340196474229\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-- Step: 1800\n",
            "Eval Val loss: 0.11681494832512535\n",
            "Eval Val accuracy: 0.9414765100671141\n",
            "Eval Val micro f1 score: 0.9442019740817833\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training loss: 0.10067012763151752\n",
            "Eval Val loss: 0.13876793341553278\n",
            "Eval Val accuracy: 0.933020134228188\n",
            "Eval Val micro f1 score: 0.9359586049324643\n",
            "Epoch 1/50: F1 Score 0.9359586049324643\n",
            "--- Best Model. Val loss: 0.9203825946382863 -> 0.9359586049324643\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "72099dc3a5734858bd059f0706bedda8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Epoch 2:   0%|          | 0/1863 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-- Step: 600\n",
            "Eval Val loss: 0.12654743053652587\n",
            "Eval Val accuracy: 0.9394630872483222\n",
            "Eval Val micro f1 score: 0.9429877638832863\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-- Step: 1200\n",
            "Eval Val loss: 0.12908332972975184\n",
            "Eval Val accuracy: 0.9350335570469799\n",
            "Eval Val micro f1 score: 0.9380198551113497\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-- Step: 1800\n",
            "Eval Val loss: 0.13774068814816248\n",
            "Eval Val accuracy: 0.936510067114094\n",
            "Eval Val micro f1 score: 0.93906858139847\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training loss: 0.06113789031985433\n",
            "Eval Val loss: 0.13144704160833123\n",
            "Eval Val accuracy: 0.9395973154362416\n",
            "Eval Val micro f1 score: 0.9425811644754494\n",
            "Epoch 2/50: F1 Score 0.9425811644754494\n",
            "--- Best Model. Val loss: 0.9359586049324643 -> 0.9425811644754494\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "78b72b59325d46d1bbe4b4538b1413ce",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Epoch 3:   0%|          | 0/1863 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-- Step: 600\n",
            "Eval Val loss: 0.14087343183266152\n",
            "Eval Val accuracy: 0.9404026845637584\n",
            "Eval Val micro f1 score: 0.9434595756110664\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-- Step: 1200\n",
            "Eval Val loss: 0.14586633916713435\n",
            "Eval Val accuracy: 0.9363758389261745\n",
            "Eval Val micro f1 score: 0.9394285714285715\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-- Step: 1800\n",
            "Eval Val loss: 0.14429452188428923\n",
            "Eval Val accuracy: 0.9350335570469799\n",
            "Eval Val micro f1 score: 0.9394750937332619\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training loss: 0.04270804004866922\n",
            "Eval Val loss: 0.14233154535009784\n",
            "Eval Val accuracy: 0.9428187919463087\n",
            "Eval Val micro f1 score: 0.9455325999463375\n",
            "Epoch 3/50: F1 Score 0.9455325999463375\n",
            "--- Best Model. Val loss: 0.9425811644754494 -> 0.9455325999463375\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2ffd7394bf5d417584f59a4da689841c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Epoch 4:   0%|          | 0/1863 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-- Step: 600\n",
            "Eval Val loss: 0.1480240217034551\n",
            "Eval Val accuracy: 0.9373154362416107\n",
            "Eval Val micro f1 score: 0.9407332660612178\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-- Step: 1200\n",
            "Eval Val loss: 0.14401367281814897\n",
            "Eval Val accuracy: 0.9393288590604026\n",
            "Eval Val micro f1 score: 0.9431123648330983\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-- Step: 1800\n",
            "Eval Val loss: 0.17118132765436858\n",
            "Eval Val accuracy: 0.9302013422818792\n",
            "Eval Val micro f1 score: 0.9331274338659863\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training loss: 0.033255181854729\n",
            "Eval Val loss: 0.12615013666537914\n",
            "Eval Val accuracy: 0.9408053691275168\n",
            "Eval Val micro f1 score: 0.945755192579149\n",
            "Epoch 4/50: F1 Score 0.945755192579149\n",
            "--- Best Model. Val loss: 0.9455325999463375 -> 0.945755192579149\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1d9cabfbf05845edb0fa449332735433",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Epoch 5:   0%|          | 0/1863 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-- Step: 600\n",
            "Eval Val loss: 0.15993051683977885\n",
            "Eval Val accuracy: 0.9306040268456376\n",
            "Eval Val micro f1 score: 0.9340231120666488\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-- Step: 1200\n",
            "Eval Val loss: 0.13013128434680601\n",
            "Eval Val accuracy: 0.941744966442953\n",
            "Eval Val micro f1 score: 0.9452642876308023\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-- Step: 1800\n",
            "Eval Val loss: 0.15846892555974737\n",
            "Eval Val accuracy: 0.9332885906040268\n",
            "Eval Val micro f1 score: 0.9374454075119264\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training loss: 0.025612791469056956\n",
            "Eval Val loss: 0.16185120900914518\n",
            "Eval Val accuracy: 0.9361073825503355\n",
            "Eval Val micro f1 score: 0.9394000402657541\n",
            "Epoch 5/50: F1 Score 0.9394000402657541\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "72671dd83c1c4e34945ee6fed2d8ea99",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Epoch 6:   0%|          | 0/1863 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-- Step: 600\n",
            "Eval Val loss: 0.19050000638120443\n",
            "Eval Val accuracy: 0.934496644295302\n",
            "Eval Val micro f1 score: 0.9364844903988183\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-- Step: 1200\n",
            "Eval Val loss: 0.1642497205304116\n",
            "Eval Val accuracy: 0.9375838926174497\n",
            "Eval Val micro f1 score: 0.9403786759769034\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-- Step: 1800\n",
            "Eval Val loss: 0.1570112535345534\n",
            "Eval Val accuracy: 0.9385234899328859\n",
            "Eval Val micro f1 score: 0.9421020778696791\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training loss: 0.021349093339662655\n",
            "Eval Val loss: 0.1866218883180971\n",
            "Eval Val accuracy: 0.9280536912751678\n",
            "Eval Val micro f1 score: 0.9311039484286866\n",
            "Epoch 6/50: F1 Score 0.9311039484286866\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7accb31ea2624626ab632f0f88eeb437",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Epoch 7:   0%|          | 0/1863 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-- Step: 600\n",
            "Eval Val loss: 0.21442883589083644\n",
            "Eval Val accuracy: 0.9343624161073826\n",
            "Eval Val micro f1 score: 0.9357588776263678\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-- Step: 1200\n",
            "Eval Val loss: 0.16983743382862904\n",
            "Eval Val accuracy: 0.9385234899328859\n",
            "Eval Val micro f1 score: 0.9410500872834698\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-- Step: 1800\n",
            "Eval Val loss: 0.18036202749271393\n",
            "Eval Val accuracy: 0.9339597315436242\n",
            "Eval Val micro f1 score: 0.9359903381642514\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training loss: 0.017991524802422016\n",
            "Eval Val loss: 0.14725358019779783\n",
            "Eval Val accuracy: 0.9375838926174497\n",
            "Eval Val micro f1 score: 0.9409787577305727\n",
            "Epoch 7/50: F1 Score 0.9409787577305727\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3de7577819f1437e9ee31e3f3ce21cca",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Epoch 8:   0%|          | 0/1863 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-- Step: 600\n",
            "Eval Val loss: 0.16450013944433498\n",
            "Eval Val accuracy: 0.9391946308724832\n",
            "Eval Val micro f1 score: 0.9412948674941294\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-- Step: 1200\n",
            "Eval Val loss: 0.18379264907242407\n",
            "Eval Val accuracy: 0.9363758389261745\n",
            "Eval Val micro f1 score: 0.9395342594456748\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-- Step: 1800\n",
            "Eval Val loss: 0.14738812655509778\n",
            "Eval Val accuracy: 0.9441610738255034\n",
            "Eval Val micro f1 score: 0.9457374740089877\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training loss: 0.012949184823148373\n",
            "Eval Val loss: 0.151411576942134\n",
            "Eval Val accuracy: 0.9465771812080537\n",
            "Eval Val micro f1 score: 0.9481590771913353\n",
            "Epoch 8/50: F1 Score 0.9481590771913353\n",
            "--- Best Model. Val loss: 0.945755192579149 -> 0.9481590771913353\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "53dcb0c1c74442efa32840a2a5052e44",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Epoch 9:   0%|          | 0/1863 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        }
      ],
      "source": [
        "train_data_sent = TransformerDataset(X_train_sent, np.vstack(y_train_sent.values).astype(np.float))\n",
        "val_data_sent = TransformerDataset(X_test_sent, np.vstack(y_test_sent.values).astype(np.float))\n",
        "\n",
        "train_dataloader_sent = DataLoader(train_data_sent, batch_size=config.BATCH_SIZE)\n",
        "val_dataloader_sent = DataLoader(val_data_sent, batch_size=config.BATCH_SIZE)\n",
        "\n",
        "best_model, best_val_micro_f1_score = run(sentiment_analysis, train_dataloader_sent, val_dataloader_sent, 'sentiment_model')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Political Analysis"
      ],
      "metadata": {
        "id": "BoTFJxhvk-SP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get the training data"
      ],
      "metadata": {
        "id": "zcWoPwznk-SP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('./ExtractedTweets.csv')\n",
        "X_pol = df['Tweet']\n",
        "y_pol = pd.DataFrame(df['Party'])"
      ],
      "metadata": {
        "id": "IwVLn9ORk-SQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pre-process the training data"
      ],
      "metadata": {
        "id": "gBjyrlBnk-SQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a column for each sentiment. \n",
        "# Each party will be mapped for an individual node in the final layer.\n",
        "y_pol['Democrat'] = int(y_pol['Party'] == 'Democrat')\n",
        "y_pol['Republican'] = int(y_pol['Party'] == 'Republican')\n",
        "y_pol = y_pol.drop('Party', axis = 1)"
      ],
      "metadata": {
        "id": "W94xEPqRk-SQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Split dataset into train and test"
      ],
      "metadata": {
        "id": "eap0n-6Zk-SR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_pol, X_test_pol, y_train_pol, y_test_pol = train_test_split(X_pol, y_pol, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "AM8rywEzk-SR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train the model"
      ],
      "metadata": {
        "id": "u-KQH8msk-SR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transformer_weights = AutoModel.from_pretrained(\n",
        "  config.MODEL_PATH\n",
        ")\n",
        "political_model = TransformerModel(transformer_weights, config.NUMBER_POLITICAL_PARTIES)"
      ],
      "metadata": {
        "id": "6BofQ8dck-SS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the model on GPU\n",
        "political_model.to(device);"
      ],
      "metadata": {
        "id": "EMC3asXTk-SS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "bc4458cfa8a54bf59a274d88144e53f1",
            "698a915789aa4da4a580fc7b2ca97d6d",
            "0fdeabad393b49d4a9809e6a2321221b",
            "f262bcab81a246e782b0c99ad9e97e64",
            "d91f2397d1a74ba98dee6de726e84aec",
            "e25c8acb4fe5474389e4ec63e0ee838a",
            "a11ea84257ea4f9b8161dfd27c9f485a",
            "4d136f32e87f45b284e56f54d78754f0",
            "b0cb50f355e5424bbbedbdfbe6ebb5bb",
            "555e9b8beefe44789c7396ebca93cf99",
            "f5dd4872aeca4887a1882c8cbdedaced",
            "56b16e5dbaf84bb28303ff762917c432",
            "454cf990719f4603aad8a85f7e039f97",
            "880f586d5d8242718b564ea1e7b9be47",
            "b65f93ec7ecb477d8533c5c34444242e",
            "f6562bcb485b4b699d9ebc0f586a6ffd",
            "2c76887ffc7f4324a3b8916409c08161",
            "17e42a972f66420194bbba3c60f1c7fe",
            "9dac6af30ef642adbd0d564535c942e1",
            "2077c3855d0149409356d97349c0b18e",
            "10642db925d249c58e0d7cc6d1014dce",
            "544f989db6b14308a2b1f108cdd4da1a",
            "e6014136171446b2a126d6bc00e9f6ed",
            "a930bf3ad5024a7b80e647d170cadd7d",
            "1adf44fd61574fc79a9039c2907647d8",
            "f708ca9761004872a28a543bb0a9e602",
            "544f4e52ed1f4724b28ed392d1b7e226",
            "3d80c36e465e48a0a431aa6565ceca02",
            "87facebdff504db68bbea0f9301582aa",
            "190dbf856fe54df7bcdcf796983504eb",
            "807471614dc84682826b7a7de485c5be",
            "c1bd1bac5eaf4539b71c0e2dee35c073",
            "ee2f00430fed4652aefa7457b4ca183c",
            "c46c186a47864260a05df2d192baf8ba",
            "bbc29894c1b4417f9332973ad4485a9e",
            "08178325d5594a3cbfbc7fc03bfdb804",
            "767706889ec24f339ae7a781343faa85",
            "91d09edf33364ad190115eb33fae786f",
            "a2f138105ea149e999bf144c81bb01b8",
            "bfcf369621124d7d8c2d0250959aa352",
            "31504fc000da466fabd4313bb80ccc8b",
            "a04f67c9554348bbb18b50bd5f198767",
            "060f69b93d314793b841b34ec2ec3328",
            "fa9daa5188f1480ea697615101d5efa3",
            "26633c6d00614dda9d1d118ddbbc2c89",
            "f7194e4b32eb496da51295e165d5f755",
            "6794ed8d44d543ba872c874cbf8c9261",
            "8776080f61ed499298f2f0f64bf2ac0f",
            "e5a25188e3f14f51ab69aa76ed2bc267",
            "e5674148502b4a86b6447c27c4eb1997",
            "c83aceb7c22d4c289f5d0f1d4b7d048b",
            "00903b4f488047bb87f873099283cc46",
            "7734ca91174e454d805c16b661a11b78",
            "2297d9527b1b4529adddec78dbe43c34",
            "9d5b278828b943ea9062b48534a580e5",
            "c1307dae44e94fae8a5e4f73d1de6f53",
            "bdb292af59ea41f8bf3aed533b62286a",
            "77d1833f5e894f2ebca515cbee3127de",
            "c059c4831e8748138103ad1cb937a471",
            "4264d697b6bd4de9b05f18c56dab22b1",
            "76b37993bdbe4c4f92a74c8bca21e212",
            "712441bbba3a4fbd89dcf949e2267b54",
            "eb5916bb848a4850a717c95daedba266",
            "7fdb98e1641a4e8fb8249451f25e6e25",
            "8d4d7c3ea7954aebaaab3e267054d18b",
            "da4f4aeccd58406facf478ffac1ec8fd"
          ]
        },
        "id": "r0uXoyX4n1P-",
        "outputId": "6b2ee9d5-c255-433e-cc24-10eb57d25be0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  \n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bc4458cfa8a54bf59a274d88144e53f1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Epoch 0:   0%|          | 0/4323 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-- Step: 1400\n",
            "Eval Val loss: 0.4768966235130832\n",
            "Eval Val accuracy: 0.7527180198935924\n",
            "Eval Val micro f1 score: 0.7562626554816315\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-- Step: 2800\n",
            "Eval Val loss: 0.4347767555029937\n",
            "Eval Val accuracy: 0.7878787878787878\n",
            "Eval Val micro f1 score: 0.78968517822555\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-- Step: 4200\n",
            "Eval Val loss: 0.40093467222607654\n",
            "Eval Val accuracy: 0.8074253990284525\n",
            "Eval Val micro f1 score: 0.8086041401642188\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training loss: 0.4795091863710407\n",
            "Eval Val loss: 0.40278075822903864\n",
            "Eval Val accuracy: 0.8079458709229702\n",
            "Eval Val micro f1 score: 0.809215470890906\n",
            "Epoch 0/50: F1 Score 0.809215470890906\n",
            "--- Best Model. Val loss: -inf -> 0.809215470890906\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "56b16e5dbaf84bb28303ff762917c432",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Epoch 1:   0%|          | 0/4323 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-- Step: 1400\n",
            "Eval Val loss: 0.4239306390512938\n",
            "Eval Val accuracy: 0.8056904927133934\n",
            "Eval Val micro f1 score: 0.80661481974038\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-- Step: 2800\n",
            "Eval Val loss: 0.43602336941081876\n",
            "Eval Val accuracy: 0.8088711542910016\n",
            "Eval Val micro f1 score: 0.8092869947377551\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-- Step: 4200\n",
            "Eval Val loss: 0.41410983115724126\n",
            "Eval Val accuracy: 0.8138445523941708\n",
            "Eval Val micro f1 score: 0.814379916705229\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training loss: 0.31205980414311396\n",
            "Eval Val loss: 0.42216208715002146\n",
            "Eval Val accuracy: 0.8185866296553319\n",
            "Eval Val micro f1 score: 0.8189705031810296\n",
            "Epoch 1/50: F1 Score 0.8189705031810296\n",
            "--- Best Model. Val loss: 0.809215470890906 -> 0.8189705031810296\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e6014136171446b2a126d6bc00e9f6ed",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Epoch 2:   0%|          | 0/4323 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-- Step: 1400\n",
            "Eval Val loss: 0.49539794254045205\n",
            "Eval Val accuracy: 0.813324080499653\n",
            "Eval Val micro f1 score: 0.8135230493377292\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-- Step: 2800\n",
            "Eval Val loss: 0.5189736401860384\n",
            "Eval Val accuracy: 0.8141915336571826\n",
            "Eval Val micro f1 score: 0.8144013880855986\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-- Step: 4200\n",
            "Eval Val loss: 0.5120922061679307\n",
            "Eval Val accuracy: 0.820205875549387\n",
            "Eval Val micro f1 score: 0.8206447882029781\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training loss: 0.1995547988671545\n",
            "Eval Val loss: 0.5203604416018843\n",
            "Eval Val accuracy: 0.8140180430256766\n",
            "Eval Val micro f1 score: 0.8143691791519638\n",
            "Epoch 2/50: F1 Score 0.8143691791519638\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c46c186a47864260a05df2d192baf8ba",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Epoch 3:   0%|          | 0/4323 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-- Step: 1400\n",
            "Eval Val loss: 0.6085045073133145\n",
            "Eval Val accuracy: 0.8177770067083044\n",
            "Eval Val micro f1 score: 0.8179163172657086\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-- Step: 2800\n",
            "Eval Val loss: 0.5556214685947268\n",
            "Eval Val accuracy: 0.8126879481841314\n",
            "Eval Val micro f1 score: 0.8130349294471432\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-- Step: 4200\n",
            "Eval Val loss: 0.5490245447570766\n",
            "Eval Val accuracy: 0.8191071015498497\n",
            "Eval Val micro f1 score: 0.8192227619708535\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training loss: 0.13566115780088514\n",
            "Eval Val loss: 0.5328339909114612\n",
            "Eval Val accuracy: 0.8170830441822808\n",
            "Eval Val micro f1 score: 0.8172329044383403\n",
            "Epoch 3/50: F1 Score 0.8172329044383403\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "26633c6d00614dda9d1d118ddbbc2c89",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Epoch 4:   0%|          | 0/4323 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-- Step: 1400\n",
            "Eval Val loss: 0.6636190922852436\n",
            "Eval Val accuracy: 0.8206685172334027\n",
            "Eval Val micro f1 score: 0.8208079114015556\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-- Step: 2800\n",
            "Eval Val loss: 0.6492259981038323\n",
            "Eval Val accuracy: 0.816736062919269\n",
            "Eval Val micro f1 score: 0.8168729038973054\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-- Step: 4200\n",
            "Eval Val loss: 0.6993811901957727\n",
            "Eval Val accuracy: 0.813439740920657\n",
            "Eval Val micro f1 score: 0.8136151999768646\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training loss: 0.09404740448985223\n",
            "Eval Val loss: 0.6203218648739153\n",
            "Eval Val accuracy: 0.814943326393708\n",
            "Eval Val micro f1 score: 0.8150140250426535\n",
            "Epoch 4/50: F1 Score 0.8150140250426535\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c1307dae44e94fae8a5e4f73d1de6f53",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Epoch 5:   0%|          | 0/4323 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-- Step: 1400\n",
            "Eval Val loss: 0.708330737668323\n",
            "Eval Val accuracy: 0.8170252139717789\n",
            "Eval Val micro f1 score: 0.8170488390249545\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-- Step: 2800\n",
            "Eval Val loss: 0.6623085879653547\n",
            "Eval Val accuracy: 0.8157529493407356\n",
            "Eval Val micro f1 score: 0.8159157943438783\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        }
      ],
      "source": [
        "train_data_pol = TransformerDataset(X_train_pol, np.vstack(y_train_pol.values).astype(np.float))\n",
        "val_data_pol = TransformerDataset(X_test_pol, np.vstack(y_test_pol.values).astype(np.float))\n",
        "\n",
        "train_dataloader_pol = DataLoader(train_data_pol, batch_size=config.BATCH_SIZE)\n",
        "val_dataloader_pol = DataLoader(val_data_pol, batch_size=config.BATCH_SIZE)\n",
        "\n",
        "best_model, best_val_micro_f1_score = run(political_model, train_dataloader_pol, val_dataloader_pol, 'political_model')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Comment**: For both models, the test accuracy and F1-score are quite high (sentiment analysis has an accuracy of 0.94 and for political classification ~0.82)"
      ],
      "metadata": {
        "id": "GZCdJ5ZJmdwK"
      }
    }
  ]
}